# RAG-Powered Website Chatbot

A complete, modular Retrieval-Augmented Generation (RAG) chatbot system for website content with three implementation phases.

## ğŸš€ Features

- **Phase 1**: Manual text ingestion and similarity search
- **Phase 2**: Automatic website scraping and AI summarization  
- **Phase 3**: Reusable, plug-and-play chatbot system
- **FAISS Vector Storage**: Efficient similarity search
- **Hugging Face Models**: Text summarization and embeddings
- **Modular Architecture**: Easy to extend and customize

## ğŸ“ Project Structure

```
rag_chatbot/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py              # Configuration settings
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                   # Raw data storage
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/             # Data ingestion modules
â”‚   â”‚   â”œâ”€â”€ manual_ingestion.py
â”‚   â”‚   â”œâ”€â”€ automatic_ingestion.py
â”‚   â”‚   â””â”€â”€ web_scraper.py
â”‚   â”œâ”€â”€ processing/            # Text processing modules
â”‚   â”‚   â”œâ”€â”€ text_processor.py
â”‚   â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”‚   â””â”€â”€ summarizer.py
â”‚   â”œâ”€â”€ vector_store/          # Vector storage modules
â”‚   â”‚   â””â”€â”€ faiss_store.py
â”‚   â”œâ”€â”€ chatbot/               # Reusable chatbot system
â”‚   â”‚   â””â”€â”€ rag_chatbot.py
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ main_phase1.py         # Phase 1 demo
â”‚   â”œâ”€â”€ main_phase2.py         # Phase 2 demo
â”‚   â”œâ”€â”€ main_phase3.py         # Phase 3 demo
â”‚   â””â”€â”€ demo.py                # Complete interactive demo
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ› ï¸ Installation

1. **Install Python 3.8+**
2. **Clone/Download the project**
3. **Install dependencies:**
   ```bash
   cd rag_chatbot
   pip install -r requirements.txt
   ```

## ğŸ¯ Quick Start

### Option 1: Interactive Demo (Recommended)
```bash
cd src
python demo.py
```
This will show a menu with all phases and interactive demos.

### Option 2: Run Individual Phases

**Phase 1 - Manual Text Ingestion:**
```bash
cd src
python main_phase1.py
```

**Phase 2 - Automatic Website Ingestion:**
```bash
cd src
python main_phase2.py
```

**Phase 3 - Interactive Chatbot:**
```bash
cd src
python main_phase3.py
```

## ğŸ“‹ Phase Breakdown

### Phase 1: Manual Website Content Ingestion âœ…
- âœ… Accept raw website text as input
- âœ… Clean and preprocess text
- âœ… Split content into overlapping chunks
- âœ… Generate vector embeddings
- âœ… Store in FAISS vector database
- âœ… Enable similarity search

### Phase 2: Automatic Website Content Ingestion & AI Summarization âœ…
- âœ… Accept website URL as input
- âœ… Automatically scrape main content
- âœ… Clean and preprocess extracted text
- âœ… Generate AI-powered summaries using Hugging Face
- âœ… Create chunks and vector embeddings
- âœ… Store in FAISS with summary display

### Phase 3: Reusable & Plug-and-Play Website Chatbot System âœ…
- âœ… Design as reusable module
- âœ… Dynamic content loading (text or URL)
- âœ… Automatic vector store rebuilding
- âœ… RAG-based question answering
- âœ… Content-based responses (no hallucinations)
- âœ… Easy integration as support chatbot

## ğŸ’» Usage Examples

### Using the Reusable Chatbot
```python
from chatbot.rag_chatbot import RAGChatbot

# Initialize chatbot
chatbot = RAGChatbot()

# Load from text
chatbot.load_from_text("Your website content here...", "Source Name")

# Or load from URL
chatbot.load_from_url("https://example.com", max_pages=3)

# Ask questions
result = chatbot.ask("What is this about?")
print(result['answer'])
```

### Manual Ingestion
```python
from ingestion.manual_ingestion import ManualIngestor

ingestor = ManualIngestor()
ingestor.ingest_text(your_text)
ingestor.save_index()
results = ingestor.query("Your question")
```

### Automatic Website Ingestion
```python
from ingestion.automatic_ingestion import AutomaticIngestor

ingestor = AutomaticIngestor()
summary, success = ingestor.ingest_website("https://example.com")
if success:
    print(f"Summary: {summary}")
    results = ingestor.query("Your question")
```

## ğŸ”§ Configuration

Edit `config/config.py` to customize:
- Chunk size and overlap
- Embedding models
- FAISS index paths
- Model parameters

## ğŸ“Š Performance Notes

- **First run**: Downloads embedding models (~500MB)
- **Subsequent runs**: Uses cached models
- **Memory usage**: Depends on content size
- **Speed**: Fast similarity search with FAISS

## ğŸ› Troubleshooting

**Import Errors**: Make sure you're running from the `src/` directory
```bash
cd src
python your_script.py
```

**Model Download Issues**: Check internet connection for first-time downloads
**Memory Issues**: Reduce `CHUNK_SIZE` in config for large documents

## ğŸš€ Next Steps

- Add more sophisticated LLM integration
- Implement web interface (Flask/FastAPI)
- Add support for document formats (PDF, DOCX)
- Implement conversation memory
- Add multi-language support

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

---

**Built with â¤ï¸ using LangChain, FAISS, and Hugging Face**
